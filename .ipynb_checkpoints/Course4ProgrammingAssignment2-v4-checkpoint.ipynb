{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1a16d3a2b4b78bd0c8a054524d667d1c",
     "grade": false,
     "grade_id": "cell-3a093c227c1a8513",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from rl_glue import RLGlue\n",
    "from environment import BaseEnvironment\n",
    "from lunar_lander import LunarLanderEnvironment\n",
    "from agent import BaseAgent\n",
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import os \n",
    "import shutil\n",
    "from plot_script import plot_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d10feeabf000214a0f53c5dfc5812437",
     "grade": false,
     "grade_id": "cell-e6d82e74c686dbf5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class ActionValueNetwork:\n",
    "    # Work Required: Yes. Fill in the layer_sizes member variable (~1 Line).\n",
    "    def __init__(self, network_config):\n",
    "        self.state_dim = network_config.get(\"state_dim\")\n",
    "        self.num_hidden_units = network_config.get(\"num_hidden_units\")\n",
    "        self.num_actions = network_config.get(\"num_actions\")\n",
    "        \n",
    "        self.rand_generator = np.random.RandomState(network_config.get(\"seed\"))\n",
    "        \n",
    "        # Specify self.layer_sizes which shows the number of nodes in each layer\n",
    "        # your code here\n",
    "        self.layer_sizes=[self.state_dim,self.num_hidden_units,self.num_actions]\n",
    "        \n",
    "        # Initialize the weights of the neural network\n",
    "        # self.weights is an array of dictionaries with each dictionary corresponding to \n",
    "        # the weights from one layer to the next. Each dictionary includes W and b\n",
    "        self.weights = [dict() for i in range(0, len(self.layer_sizes) - 1)]\n",
    "        for i in range(0, len(self.layer_sizes) - 1):\n",
    "            self.weights[i]['W'] = self.init_saxe(self.layer_sizes[i], self.layer_sizes[i + 1])\n",
    "            self.weights[i]['b'] = np.zeros((1, self.layer_sizes[i + 1]))\n",
    "    \n",
    "    # Work Required: No.\n",
    "    def get_action_values(self, s):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            s (Numpy array): The state.\n",
    "        Returns:\n",
    "            The action-values (Numpy array) calculated using the network's weights.\n",
    "        \"\"\"\n",
    "        \n",
    "        W0, b0 = self.weights[0]['W'], self.weights[0]['b']\n",
    "        psi = np.dot(s, W0) + b0\n",
    "        x = np.maximum(psi, 0)\n",
    "        \n",
    "        W1, b1 = self.weights[1]['W'], self.weights[1]['b']\n",
    "        q_vals = np.dot(x, W1) + b1\n",
    "\n",
    "        return q_vals\n",
    "    \n",
    "    # Work Required: No.\n",
    "    def get_TD_update(self, s, delta_mat):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            s (Numpy array): The state.\n",
    "            delta_mat (Numpy array): A 2D array of shape (batch_size, num_actions). Each row of delta_mat  \n",
    "            correspond to one state in the batch. Each row has only one non-zero element \n",
    "            which is the TD-error corresponding to the action taken.\n",
    "        Returns:\n",
    "            The TD update (Array of dictionaries with gradient times TD errors) for the network's weights\n",
    "        \"\"\"\n",
    "\n",
    "        W0, b0 = self.weights[0]['W'], self.weights[0]['b']\n",
    "        W1, b1 = self.weights[1]['W'], self.weights[1]['b']\n",
    "        \n",
    "        psi = np.dot(s, W0) + b0\n",
    "        x = np.maximum(psi, 0)\n",
    "        dx = (psi > 0).astype(float)\n",
    "\n",
    "        # td_update has the same structure as self.weights, that is an array of dictionaries.\n",
    "        # td_update[0][\"W\"], td_update[0][\"b\"], td_update[1][\"W\"], and td_update[1][\"b\"] have the same shape as \n",
    "        # self.weights[0][\"W\"], self.weights[0][\"b\"], self.weights[1][\"W\"], and self.weights[1][\"b\"] respectively\n",
    "        td_update = [dict() for i in range(len(self.weights))]\n",
    "         \n",
    "        v = delta_mat\n",
    "        td_update[1]['W'] = np.dot(x.T, v) * 1. / s.shape[0]\n",
    "        td_update[1]['b'] = np.sum(v, axis=0, keepdims=True) * 1. / s.shape[0]\n",
    "        \n",
    "        v = np.dot(v, W1.T) * dx\n",
    "        td_update[0]['W'] = np.dot(s.T, v) * 1. / s.shape[0]\n",
    "        td_update[0]['b'] = np.sum(v, axis=0, keepdims=True) * 1. / s.shape[0]\n",
    "                \n",
    "        return td_update\n",
    "    \n",
    "    # Work Required: No. You may wish to read the relevant paper for more information on this weight initialization\n",
    "    # (Exact solutions to the nonlinear dynamics of learning in deep linear neural networks by Saxe, A et al., 2013)\n",
    "    def init_saxe(self, rows, cols):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            rows (int): number of input units for layer.\n",
    "            cols (int): number of output units for layer.\n",
    "        Returns:\n",
    "            NumPy Array consisting of weights for the layer based on the initialization in Saxe et al.\n",
    "        \"\"\"\n",
    "        tensor = self.rand_generator.normal(0, 1, (rows, cols))\n",
    "        if rows < cols:\n",
    "            tensor = tensor.T\n",
    "        tensor, r = np.linalg.qr(tensor)\n",
    "        d = np.diag(r, 0)\n",
    "        ph = np.sign(d)\n",
    "        tensor *= ph\n",
    "\n",
    "        if rows < cols:\n",
    "            tensor = tensor.T\n",
    "        return tensor\n",
    "    \n",
    "    # Work Required: No.\n",
    "    def get_weights(self):\n",
    "        \"\"\"\n",
    "        Returns: \n",
    "            A copy of the current weights of this network.\n",
    "        \"\"\"\n",
    "        return deepcopy(self.weights)\n",
    "    \n",
    "    # Work Required: No.\n",
    "    def set_weights(self, weights):\n",
    "        \"\"\"\n",
    "        Args: \n",
    "            weights (list of dictionaries): Consists of weights that this network will set as its own weights.\n",
    "        \"\"\"\n",
    "        self.weights = deepcopy(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "798d4618ba32342f63eb237947151a4a",
     "grade": false,
     "grade_id": "cell-585fd403a17cf660",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Adam():\n",
    "    # Work Required: Yes. Fill in the initialization for self.m and self.v (~4 Lines).\n",
    "    def __init__(self, layer_sizes, \n",
    "                 optimizer_info):\n",
    "        self.layer_sizes = layer_sizes\n",
    "\n",
    "        # Specify Adam algorithm's hyper parameters\n",
    "        self.step_size = optimizer_info.get(\"step_size\")\n",
    "        self.beta_m = optimizer_info.get(\"beta_m\")\n",
    "        self.beta_v = optimizer_info.get(\"beta_v\")\n",
    "        self.epsilon = optimizer_info.get(\"epsilon\")\n",
    "        \n",
    "        # Initialize Adam algorithm's m and v\n",
    "        self.m = [dict() for i in range(1, len(self.layer_sizes))]\n",
    "        self.v = [dict() for i in range(1, len(self.layer_sizes))]\n",
    "        \n",
    "        for i in range(0, len(self.layer_sizes) - 1):\n",
    "            # Hint: The initialization for m and v should look very much like the initializations of the weights\n",
    "            # except for the fact that initialization here is to zeroes (see description above.)\n",
    "            # Replace the None in each following line\n",
    "\n",
    "            self.m[i][\"W\"] = np.zeros((layer_sizes[i],layer_sizes[i+1]))\n",
    "            self.m[i][\"b\"] = np.zeros((1,layer_sizes[i+1]))\n",
    "            self.v[i][\"W\"] = np.zeros((layer_sizes[i],layer_sizes[i+1]))\n",
    "            self.v[i][\"b\"] = np.zeros((1,layer_sizes[i+1]))\n",
    "            \n",
    "            # your code here\n",
    "            \n",
    "            \n",
    "        # Notice that to calculate m_hat and v_hat, we use powers of beta_m and beta_v to \n",
    "        # the time step t. We can calculate these powers using an incremental product. At initialization then, \n",
    "        # beta_m_product and beta_v_product should be ...? (Note that timesteps start at 1 and if we were to \n",
    "        # start from 0, the denominator would be 0.)\n",
    "        self.beta_m_product = self.beta_m\n",
    "        self.beta_v_product = self.beta_v\n",
    "    \n",
    "    # Work Required: Yes. Fill in the weight updates (~5-7 lines).\n",
    "    def update_weights(self, weights, td_errors_times_gradients):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            weights (Array of dictionaries): The weights of the neural network.\n",
    "            td_errors_times_gradients (Array of dictionaries): The gradient of the \n",
    "            action-values with respect to the network's weights times the TD-error\n",
    "        Returns:\n",
    "            The updated weights (Array of dictionaries).\n",
    "        \"\"\"\n",
    "        for i in range(len(weights)):\n",
    "            for param in weights[i].keys():\n",
    "                # Hint: Follow the equations above. First, you should update m and v and then compute \n",
    "                # m_hat and v_hat. Finally, compute how much the weights should be incremented by.\n",
    "                # self.m[i][param] = None\n",
    "                # self.v[i][param] = None\n",
    "                # m_hat = None\n",
    "                # v_hat = None\n",
    "                self.m[i][param]=(self.beta_m*self.m[i][param])+((1-self.beta_m)*td_errors_times_gradients[i][param])\n",
    "                self.v[i][param]=(self.beta_v*self.v[i][param])+((1-self.beta_v)*(td_errors_times_gradients[i][param]**2))\n",
    "               \n",
    "                m_hat=self.m[i][param]/(1-self.beta_m_product)\n",
    "                v_hat=self.v[i][param]/(1-self.beta_v_product)\n",
    "                weight_update=self.step_size*(m_hat)/(np.sqrt(v_hat)+self.epsilon)\n",
    "                # your code here\n",
    "                \n",
    "                \n",
    "                weights[i][param] = weights[i][param] + weight_update\n",
    "        # Notice that to calculate m_hat and v_hat, we use powers of beta_m and beta_v to \n",
    "        ### update self.beta_m_product and self.beta_v_product\n",
    "        self.beta_m_product *= self.beta_m\n",
    "        self.beta_v_product *= self.beta_v\n",
    "        \n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dd216bf2169746f6331d6a5fbd79d605",
     "grade": false,
     "grade_id": "cell-1e1aaa0d442eb015",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# ---------------\n",
    "# Discussion Cell\n",
    "# ---------------\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, size, minibatch_size, seed):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            size (integer): The size of the replay buffer.              \n",
    "            minibatch_size (integer): The sample size.\n",
    "            seed (integer): The seed for the random number generator. \n",
    "        \"\"\"\n",
    "        self.buffer = []\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.rand_generator = np.random.RandomState(seed)\n",
    "        self.max_size = size\n",
    "\n",
    "    def append(self, state, action, reward, terminal, next_state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state (Numpy array): The state.              \n",
    "            action (integer): The action.\n",
    "            reward (float): The reward.\n",
    "            terminal (integer): 1 if the next state is a terminal state and 0 otherwise.\n",
    "            next_state (Numpy array): The next state.           \n",
    "        \"\"\"\n",
    "        if len(self.buffer) == self.max_size:\n",
    "            del self.buffer[0]\n",
    "        self.buffer.append([state, action, reward, terminal, next_state])\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            A list of transition tuples including state, action, reward, terinal, and next_state\n",
    "        \"\"\"\n",
    "        idxs = self.rand_generator.choice(np.arange(len(self.buffer)), size=self.minibatch_size)\n",
    "        return [self.buffer[idx] for idx in idxs]\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0bc082ff0d5b933fb88fa1936f2057d3",
     "grade": false,
     "grade_id": "cell-b32ebbeb60c5b9f7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def softmax(action_values, tau=1.0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        action_values (Numpy array): A 2D array of shape (batch_size, num_actions). \n",
    "                       The action-values computed by an action-value network.              \n",
    "        tau (float): The temperature parameter scalar.\n",
    "    Returns:\n",
    "        A 2D array of shape (batch_size, num_actions). Where each column is a probability distribution over\n",
    "        the actions representing the policy.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the preferences by dividing the action-values by the temperature parameter tau\n",
    "    preferences = action_values/tau\n",
    "    # Compute the maximum preference across the actions\n",
    "    max_preference = np.max(preferences,axis=1)\n",
    "    \n",
    "    # your code here\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Reshape max_preference array which has shape [Batch,] to [Batch, 1]. This allows NumPy broadcasting \n",
    "    # when subtracting the maximum preference from the preference of each action.\n",
    "    reshaped_max_preference = max_preference.reshape((-1, 1))\n",
    "    \n",
    "    # Compute the numerator, i.e., the exponential of the preference - the max preference.\n",
    "    exp_preferences = np.exp(preferences-reshaped_max_preference)\n",
    "    # Compute the denominator, i.e., the sum over the numerator along the actions axis.\n",
    "    sum_of_exp_preferences = np.sum(exp_preferences,axis=1)\n",
    "    \n",
    "    # your code here\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Reshape sum_of_exp_preferences array which has shape [Batch,] to [Batch, 1] to  allow for NumPy broadcasting \n",
    "    # when dividing the numerator by the denominator.\n",
    "    reshaped_sum_of_exp_preferences = sum_of_exp_preferences.reshape((-1, 1))\n",
    "    \n",
    "    # Compute the action probabilities according to the equation in the previous cell.\n",
    "    action_probs = exp_preferences/reshaped_sum_of_exp_preferences\n",
    "    \n",
    "    # your code here\n",
    "    \n",
    "    \n",
    "    \n",
    "    # squeeze() removes any singleton dimensions. It is used here because this function is used in the \n",
    "    # agent policy when selecting an action (for which the batch dimension is 1.) As np.random.choice is used in \n",
    "    # the agent policy and it expects 1D arrays, we need to remove this singleton batch dimension.\n",
    "    action_probs = action_probs.squeeze()\n",
    "    return action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e8ff7f160ca26a7639acc062ae6b29a",
     "grade": false,
     "grade_id": "cell-f370691c828efad9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### Work Required: Yes. Fill in code in get_td_error (~9 Lines).\n",
    "def get_td_error(states, next_states, actions, rewards, discount, terminals, network, current_q, tau):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        states (Numpy array): The batch of states with the shape (batch_size, state_dim).\n",
    "        next_states (Numpy array): The batch of next states with the shape (batch_size, state_dim).\n",
    "        actions (Numpy array): The batch of actions with the shape (batch_size,).\n",
    "        rewards (Numpy array): The batch of rewards with the shape (batch_size,).\n",
    "        discount (float): The discount factor.\n",
    "        terminals (Numpy array): The batch of terminals with the shape (batch_size,).\n",
    "        network (ActionValueNetwork): The latest state of the network that is getting replay updates.\n",
    "        current_q (ActionValueNetwork): The fixed network used for computing the targets, \n",
    "                                        and particularly, the action-values at the next-states.\n",
    "    Returns:\n",
    "        The TD errors (Numpy array) for actions taken, of shape (batch_size,)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Note: Here network is the latest state of the network that is getting replay updates. In other words, \n",
    "    # the network represents Q_{t+1}^{i} whereas current_q represents Q_t, the fixed network used for computing the \n",
    "    # targets, and particularly, the action-values at the next-states.\n",
    "    \n",
    "    # Compute action values at next states using current_q network\n",
    "    # Note that q_next_mat is a 2D array of shape (batch_size, num_actions)\n",
    "    \n",
    "    ### START CODE HERE (~1 Line)\n",
    "    q_next_mat = current_q.get_action_values(next_states)\n",
    "    ### END CODE HERE\n",
    "    # your code here\n",
    "    policy=softmax(q_next_mat,tau)\n",
    "    \n",
    "    # Compute policy at next state by passing the action-values in q_next_mat to softmax()\n",
    "    # Note that probs_mat is a 2D array of shape (batch_size, num_actions)\n",
    "    \n",
    "    ### START CODE HERE (~1 Line)\n",
    "    probs_mat = policy\n",
    "    ### END CODE HERE\n",
    "    # your code here\n",
    "    v_next_vec=np.sum(np.multiply(probs_mat,q_next_mat),axis=1)*(1-terminals)\n",
    "\n",
    "    # Compute the estimate of the next state value, v_next_vec.\n",
    "    # Hint: sum the action-values for the next_states weighted by the policy, probs_mat. Then, multiply by\n",
    "    # (1 - terminals) to make sure v_next_vec is zero for terminal next states.\n",
    "    # Note that v_next_vec is a 1D array of shape (batch_size,)\n",
    "    \n",
    "    ### START CODE HERE (~3 Lines)\n",
    "    #v_next_vec = v_next_vec+self.step_size*(reward.reshape(-1,1))\n",
    "    ### END CODE HERE\n",
    "    # your code here\n",
    "    \n",
    "    \n",
    "    # Compute Expected Sarsa target\n",
    "    # Note that target_vec is a 1D array of shape (batch_size,)\n",
    "    \n",
    "    ### START CODE HERE (~1 Line)\n",
    "    target_vec = rewards+discount*v_next_vec\n",
    "    ### END CODE HERE\n",
    "    # your code here\n",
    "    \n",
    "    \n",
    "    # Compute action values at the current states for all actions using network\n",
    "    # Note that q_mat is a 2D array of shape (batch_size, num_actions)\n",
    "    \n",
    "    ### START CODE HERE (~1 Line)\n",
    "    q_mat = network.get_action_values(states)\n",
    "    ### END CODE HERE\n",
    "    # your code here\n",
    "    \n",
    "    \n",
    "    # Batch Indices is an array from 0 to the batch size - 1. \n",
    "    batch_indices = np.arange(q_mat.shape[0])\n",
    "\n",
    "    # Compute q_vec by selecting q(s, a) from q_mat for taken actions\n",
    "    # Use batch_indices as the index for the first dimension of q_mat\n",
    "    # Note that q_vec is a 1D array of shape (batch_size)\n",
    "    \n",
    "    ### START CODE HERE (~1 Line)\n",
    "    q_vec = q_mat[batch_indices,actions]\n",
    "    ### END CODE HERE\n",
    "    # your code here\n",
    "    \n",
    "    \n",
    "    # Compute TD errors for actions taken\n",
    "    # Note that delta_vec is a 1D array of shape (batch_size)\n",
    "    \n",
    "    ### START CODE HERE (~1 Line)\n",
    "    delta_vec = target_vec-q_vec\n",
    "    ### END CODE HERE\n",
    "    # your code here\n",
    "    \n",
    "    \n",
    "    return delta_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5772dc09af7d47867f70baa8580057cf",
     "grade": false,
     "grade_id": "cell-2b9714cb6ee933de",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# -----------\n",
    "# Graded Cell\n",
    "# -----------\n",
    "\n",
    "### Work Required: Yes. Fill in code in optimize_network (~2 Lines).\n",
    "def optimize_network(experiences, discount, optimizer, network, current_q, tau):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        experiences (Numpy array): The batch of experiences including the states, actions, \n",
    "                                   rewards, terminals, and next_states.\n",
    "        discount (float): The discount factor.\n",
    "        network (ActionValueNetwork): The latest state of the network that is getting replay updates.\n",
    "        current_q (ActionValueNetwork): The fixed network used for computing the targets, \n",
    "                                        and particularly, the action-values at the next-states.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get states, action, rewards, terminals, and next_states from experiences\n",
    "    states, actions, rewards, terminals, next_states = map(list, zip(*experiences))\n",
    "    states = np.concatenate(states)\n",
    "    next_states = np.concatenate(next_states)\n",
    "    rewards = np.array(rewards)\n",
    "    terminals = np.array(terminals)\n",
    "    batch_size = states.shape[0]\n",
    "\n",
    "    # Compute TD error using the get_td_error function\n",
    "    # Note that q_vec is a 1D array of shape (batch_size)\n",
    "    delta_vec = get_td_error(states, next_states, actions, rewards, discount, terminals, network, current_q, tau)\n",
    "\n",
    "    # Batch Indices is an array from 0 to the batch_size - 1. \n",
    "    batch_indices = np.arange(batch_size)\n",
    "\n",
    "    # Make a td error matrix of shape (batch_size, num_actions)\n",
    "    # delta_mat has non-zero value only for actions taken\n",
    "    delta_mat = np.zeros((batch_size, network.num_actions))\n",
    "    delta_mat[batch_indices, actions] = delta_vec\n",
    "\n",
    "    # Pass delta_mat to compute the TD errors times the gradients of the network's weights from back-propagation\n",
    "    \n",
    "    ### START CODE HERE\n",
    "    td_update = network.get_TD_update(states,delta_mat)\n",
    "    ### END CODE HERE\n",
    "    # your code here\n",
    "    \n",
    "    \n",
    "    # Pass network.get_weights and the td_update to the optimizer to get updated weights\n",
    "    ### START CODE HERE\n",
    "    weights = optimizer.update_weights(network.get_weights(),td_update)\n",
    "    ### END CODE HERE\n",
    "    # your code here\n",
    "    \n",
    "    \n",
    "    network.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1d9134ac89ad8c86157599044f5dbc8e",
     "grade": false,
     "grade_id": "cell-54b5db480295424c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### Work Required: Yes. Fill in code in agent_step and agent_end (~7 Lines).\n",
    "class Agent(BaseAgent):\n",
    "    def __init__(self):\n",
    "        self.name = \"expected_sarsa_agent\"\n",
    "        \n",
    "    # Work Required: No.\n",
    "    def agent_init(self, agent_config):\n",
    "        \"\"\"Setup for the agent called when the experiment first starts.\n",
    "\n",
    "        Set parameters needed to setup the agent.\n",
    "\n",
    "        Assume agent_config dict contains:\n",
    "        {\n",
    "            network_config: dictionary,\n",
    "            optimizer_config: dictionary,\n",
    "            replay_buffer_size: integer,\n",
    "            minibatch_sz: integer, \n",
    "            num_replay_updates_per_step: float\n",
    "            discount_factor: float,\n",
    "        }\n",
    "        \"\"\"\n",
    "        self.replay_buffer = ReplayBuffer(agent_config['replay_buffer_size'], \n",
    "                                          agent_config['minibatch_sz'], agent_config.get(\"seed\"))\n",
    "        self.network = ActionValueNetwork(agent_config['network_config'])\n",
    "        self.optimizer = Adam(self.network.layer_sizes, agent_config[\"optimizer_config\"])\n",
    "        self.num_actions = agent_config['network_config']['num_actions']\n",
    "        self.num_replay = agent_config['num_replay_updates_per_step']\n",
    "        self.discount = agent_config['gamma']\n",
    "        self.tau = agent_config['tau']\n",
    "        \n",
    "        self.rand_generator = np.random.RandomState(agent_config.get(\"seed\"))\n",
    "        \n",
    "        self.last_state = None\n",
    "        self.last_action = None\n",
    "        \n",
    "        self.sum_rewards = 0\n",
    "        self.episode_steps = 0\n",
    "\n",
    "    # Work Required: No.\n",
    "    def policy(self, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state (Numpy array): the state.\n",
    "        Returns:\n",
    "            the action. \n",
    "        \"\"\"\n",
    "        action_values = self.network.get_action_values(state)\n",
    "        probs_batch = softmax(action_values, self.tau)\n",
    "        action = self.rand_generator.choice(self.num_actions, p=probs_batch.squeeze())\n",
    "        return action\n",
    "\n",
    "    # Work Required: No.\n",
    "    def agent_start(self, state):\n",
    "        \"\"\"The first method called when the experiment starts, called after\n",
    "        the environment starts.\n",
    "        Args:\n",
    "            state (Numpy array): the state from the\n",
    "                environment's evn_start function.\n",
    "        Returns:\n",
    "            The first action the agent takes.\n",
    "        \"\"\"\n",
    "        self.sum_rewards = 0\n",
    "        self.episode_steps = 0\n",
    "        self.last_state = np.array([state])\n",
    "        self.last_action = self.policy(self.last_state)\n",
    "        return self.last_action\n",
    "\n",
    "    # Work Required: Yes. Fill in the action selection, replay-buffer update, \n",
    "    # weights update using optimize_network, and updating last_state and last_action (~5 lines).\n",
    "    def agent_step(self, reward, state):\n",
    "        \"\"\"A step taken by the agent.\n",
    "        Args:\n",
    "            reward (float): the reward received for taking the last action taken\n",
    "            state (Numpy array): the state from the\n",
    "                environment's step based, where the agent ended up after the\n",
    "                last step\n",
    "        Returns:\n",
    "            The action the agent is taking.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.sum_rewards += reward\n",
    "        self.episode_steps += 1\n",
    "\n",
    "        # Make state an array of shape (1, state_dim) to add a batch dimension and\n",
    "        # to later match the get_action_values() and get_TD_update() functions\n",
    "        state = np.array([state])\n",
    "\n",
    "        # Select action\n",
    "        # your code here\n",
    "        action=self.policy(state)\n",
    "        \n",
    "        # Append new experience to replay buffer\n",
    "        # Note: look at the replay_buffer append function for the order of arguments\n",
    "\n",
    "        # your code here\n",
    "        self.replay_buffer.append(self.last_state,self.last_action,reward,0,state)\n",
    "        \n",
    "        # Perform replay steps:\n",
    "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size:\n",
    "            current_q = deepcopy(self.network)\n",
    "            for _ in range(self.num_replay):\n",
    "                \n",
    "                # Get sample experiences from the replay buffer\n",
    "                experiences = self.replay_buffer.sample()\n",
    "                \n",
    "                # Call optimize_network to update the weights of the network (~1 Line)\n",
    "                # your code here\n",
    "                optimize_network(experiences,self.discount,self.optimizer,self.network,current_q,self.tau)\n",
    "                \n",
    "        # Update the last state and last action.\n",
    "        ### START CODE HERE (~2 Lines)\n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "        ### END CODE HERE\n",
    "        # your code here\n",
    "        \n",
    "        \n",
    "        return action\n",
    "\n",
    "    # Work Required: Yes. Fill in the replay-buffer update and\n",
    "    # update of the weights using optimize_network (~2 lines).\n",
    "    def agent_end(self, reward):\n",
    "        \"\"\"Run when the agent terminates.\n",
    "        Args:\n",
    "            reward (float): the reward the agent received for entering the\n",
    "                terminal state.\n",
    "        \"\"\"\n",
    "        self.sum_rewards += reward\n",
    "        self.episode_steps += 1\n",
    "        \n",
    "        # Set terminal state to an array of zeros\n",
    "        state = np.zeros_like(self.last_state)\n",
    "\n",
    "        # Append new experience to replay buffer\n",
    "        # Note: look at the replay_buffer append function for the order of arguments\n",
    "        \n",
    "        # your code here\n",
    "        self.replay_buffer.append(self.last_state,self.last_action,reward,1,state)\n",
    "        \n",
    "        # Perform replay steps:\n",
    "        if self.replay_buffer.size() > self.replay_buffer.minibatch_size:\n",
    "            current_q = deepcopy(self.network)\n",
    "            for _ in range(self.num_replay):\n",
    "                \n",
    "                # Get sample experiences from the replay buffer\n",
    "                experiences = self.replay_buffer.sample()\n",
    "                optimize_network(experiences,self.discount,self.optimizer,self.network,current_q,self.tau)\n",
    "                # Call optimize_network to update the weights of the network\n",
    "                # your code here\n",
    "                \n",
    "                \n",
    "        \n",
    "    def agent_message(self, message):\n",
    "        if message == \"get_sum_reward\":\n",
    "            return self.sum_rewards\n",
    "        else:\n",
    "            raise Exception(\"Unrecognized Message!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1500/1500 [20:16<00:00,  1.23it/s]\n"
     ]
    }
   ],
   "source": [
    "''' Editable cell'''\n",
    "\n",
    "# ---------------\n",
    "# Discussion Cell\n",
    "# ---------------\n",
    "\n",
    "def run_experiment(environment, agent, environment_parameters, agent_parameters, experiment_parameters):\n",
    "    \n",
    "    rl_glue = RLGlue(environment, agent)\n",
    "        \n",
    "    # save sum of reward at the end of each episode\n",
    "    agent_sum_reward = np.zeros((experiment_parameters[\"num_runs\"], \n",
    "                                 experiment_parameters[\"num_episodes\"]))\n",
    "\n",
    "    env_info = {}\n",
    "\n",
    "    agent_info = agent_parameters\n",
    "\n",
    "    # one agent setting\n",
    "    for run in range(1, experiment_parameters[\"num_runs\"]+1):\n",
    "        agent_info[\"seed\"] = run\n",
    "        agent_info[\"network_config\"][\"seed\"] = run\n",
    "        env_info[\"seed\"] = run\n",
    "\n",
    "        rl_glue.rl_init(agent_info, env_info)\n",
    "        \n",
    "        for episode in tqdm(range(1, experiment_parameters[\"num_episodes\"]+1)):\n",
    "            # run episode\n",
    "            rl_glue.rl_episode(experiment_parameters[\"timeout\"])\n",
    "            \n",
    "            episode_reward = rl_glue.rl_agent_message(\"get_sum_reward\")\n",
    "            agent_sum_reward[run - 1, episode - 1] = episode_reward\n",
    "    save_name = \"{}\".format(rl_glue.agent.name)\n",
    "    if not os.path.exists('results'):\n",
    "        os.makedirs('results')\n",
    "    np.save(\"results/sum_reward_{}\".format(save_name), agent_sum_reward)\n",
    "    shutil.make_archive('results', 'zip', 'results')\n",
    "    return rl_glue\n",
    "# Run Experiment\n",
    "\n",
    "# Experiment parameters\n",
    "experiment_parameters = {\n",
    "    \"num_runs\" : 1,\n",
    "    \"num_episodes\" : 1500,\n",
    "    # OpenAI Gym environments allow for a timestep limit timeout, causing episodes to end after \n",
    "    # some number of timesteps. Here we use the default of 500.\n",
    "    \"timeout\" : 500\n",
    "}\n",
    "\n",
    "# Environment parameters\n",
    "environment_parameters = {}\n",
    "\n",
    "current_env = LunarLanderEnvironment\n",
    "\n",
    "# Agent parameters\n",
    "agent_parameters = {\n",
    "    'network_config': {\n",
    "        'state_dim': 8,\n",
    "        'num_hidden_units': 256,\n",
    "        'num_actions': 4\n",
    "    },\n",
    "    'optimizer_config': {\n",
    "        'step_size': 1e-3,\n",
    "        'beta_m': 0.9, \n",
    "        'beta_v': 0.999,\n",
    "        'epsilon': 1e-8\n",
    "    },\n",
    "    'replay_buffer_size': 50000,\n",
    "    'minibatch_sz': 16,\n",
    "    'num_replay_updates_per_step': 4,\n",
    "    'gamma': 0.99,\n",
    "    'tau': 0.001\n",
    "}\n",
    "current_agent = Agent\n",
    "\n",
    "# run experiment\n",
    "rl_glue=run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3132510fde7c06020276a6c6f272eccd",
     "grade": false,
     "grade_id": "cell-337be142123eb81f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './/sum_reward_random_agent.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-dc21af669f4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplot_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"expected_sarsa_agent\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"random_agent\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Deeplearning.ai\\RL\\fresh\\plot_script.py\u001b[0m in \u001b[0;36mplot_result\u001b[1;34m(data_name_array)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;31m# load data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'sum_reward_{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0msum_reward_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}.npy'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;31m# smooth data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\obj\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    414\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 416\u001b[1;33m             \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    417\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './/sum_reward_random_agent.npy'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_result([\"expected_sarsa_agent\", \"random_agent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from PIL import Image\n",
    "from plot_script import plot_result\n",
    "from IPython.display import clear_output\n",
    "env = gym.make(\"LunarLander-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    state=env.reset()\n",
    "    done=False\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action=rl_glue.agent.policy(state)\n",
    "        n_state,reward,done,info=env.step(action)\n",
    "        state=n_state\n",
    "    done=False\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "complete-reinforcement-learning-system",
   "graded_item_id": "8dMlx",
   "launcher_item_id": "4O5gG"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
